"""
===========================================
Eye Web Backend ‚Äî URL Checker Service
===========================================

Servi√ßo para an√°lise de seguran√ßa de URLs.
Integra:
- Supabase (cache de resultados)
- Google Safe Browsing (verifica√ß√£o de amea√ßas)
- Groq/Llama 3 (an√°lise IA)

Arquitetura: Stale-While-Revalidate
- Retorna cache imediatamente (se existir)
- Re-verifica em background se cache antigo

Capacidade: ~10,000 verifica√ß√µes/dia (limite Google Safe Browsing)
"""

import hashlib
import logging
import asyncio
from datetime import datetime, timezone
from typing import Optional
from enum import Enum

import httpx
from supabase import create_client, Client

from ..config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


# ===========================================
# ENUMS E MODELOS
# ===========================================

class URLStatus(str, Enum):
    """Status de seguran√ßa do URL."""
    SAFE = "safe"
    SUSPICIOUS = "suspicious"
    MALICIOUS = "malicious"
    UNKNOWN = "unknown"
    ANALYZING = "analyzing"


# ===========================================
# SUPABASE CLIENT
# ===========================================

_supabase_client: Optional[Client] = None


def get_supabase() -> Optional[Client]:
    """Retorna cliente Supabase singleton."""
    global _supabase_client
    
    if _supabase_client is None:
        if settings.SUPABASE_URL and settings.SUPABASE_SERVICE_KEY:
            try:
                _supabase_client = create_client(
                    settings.SUPABASE_URL,
                    settings.SUPABASE_SERVICE_KEY
                )
                logger.info("‚úÖ Supabase client initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize Supabase: {e}")
                return None
        else:
            logger.warning("‚ö†Ô∏è Supabase credentials not configured")
            return None
    
    return _supabase_client


# ===========================================
# HASH UTILITIES
# ===========================================

def hash_url(url: str) -> str:
    """Gera hash SHA-256 do URL normalizado."""
    normalized = url.lower().strip().rstrip('/')
    return hashlib.sha256(normalized.encode()).hexdigest()


def normalize_url(url: str) -> str:
    """Normaliza URL para consist√™ncia."""
    url = url.strip()
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    return url.rstrip('/')


# ===========================================
# CACHE OPERATIONS (SUPABASE)
# ===========================================

async def get_cached_result(url_hash: str) -> Optional[dict]:
    """
    Busca resultado em cache no Supabase.
    
    Returns:
        Dict com resultado ou None se n√£o existir.
    """
    supabase = get_supabase()
    if not supabase:
        return None
    
    try:
        response = supabase.table("url_scans").select("*").eq("url_hash", url_hash).execute()
        
        if response.data and len(response.data) > 0:
            logger.debug(f"‚úÖ Cache hit for {url_hash[:8]}...")
            return response.data[0]
        
        logger.debug(f"‚ùå Cache miss for {url_hash[:8]}...")
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Error fetching cache: {e}")
        return None


async def save_to_cache(
    url_hash: str,
    original_url: str,
    status: URLStatus,
    ai_opinion: Optional[str] = None,
    threat_details: Optional[dict] = None
) -> bool:
    """
    Guarda ou atualiza resultado no cache (UPSERT).
    """
    supabase = get_supabase()
    if not supabase:
        return False
    
    try:
        data = {
            "url_hash": url_hash,
            "original_url": original_url,
            "status": status.value,
            "ai_opinion": ai_opinion,
            "threat_details": threat_details or {},
            "last_check": datetime.now(timezone.utc).isoformat(),
        }
        
        # UPSERT: insere ou atualiza se j√° existir
        response = supabase.table("url_scans").upsert(
            data,
            on_conflict="url_hash"
        ).execute()
        
        logger.info(f"‚úÖ Saved to cache: {url_hash[:8]}... = {status.value}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error saving to cache: {e}")
        return False


# ===========================================
# GOOGLE SAFE BROWSING
# ===========================================

async def check_google_safe_browsing(url: str) -> dict:
    """
    Verifica URL no Google Safe Browsing API.
    
    Returns:
        Dict com resultado da verifica√ß√£o.
    """
    if not settings.GOOGLE_SAFE_BROWSING_KEY:
        logger.warning("‚ö†Ô∏è Google Safe Browsing API key not configured")
        return {"checked": False, "error": "API key not configured"}
    
    api_url = f"https://safebrowsing.googleapis.com/v4/threatMatches:find?key={settings.GOOGLE_SAFE_BROWSING_KEY}"
    
    payload = {
        "client": {
            "clientId": "eyeweb-url-checker",
            "clientVersion": "1.0.0"
        },
        "threatInfo": {
            "threatTypes": [
                "MALWARE",
                "SOCIAL_ENGINEERING",
                "UNWANTED_SOFTWARE",
                "POTENTIALLY_HARMFUL_APPLICATION"
            ],
            "platformTypes": ["ANY_PLATFORM"],
            "threatEntryTypes": ["URL"],
            "threatEntries": [{"url": url}]
        }
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(api_url, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            if "matches" in data and len(data["matches"]) > 0:
                threats = [match.get("threatType", "UNKNOWN") for match in data["matches"]]
                logger.warning(f"‚ö†Ô∏è Google Safe Browsing found threats: {threats}")
                return {
                    "checked": True,
                    "is_threat": True,
                    "threats": threats,
                    "source": "google_safe_browsing"
                }
            
            logger.debug("‚úÖ Google Safe Browsing: No threats found")
            return {
                "checked": True,
                "is_threat": False,
                "source": "google_safe_browsing"
            }
            
    except httpx.TimeoutException:
        logger.error("‚ùå Google Safe Browsing timeout")
        return {"checked": False, "error": "timeout"}
    except Exception as e:
        logger.error(f"‚ùå Google Safe Browsing error: {e}")
        return {"checked": False, "error": str(e)}


# ===========================================
# URLSCAN.IO (DESATIVADO)
# ===========================================
# Removido para aumentar capacidade de 100 ‚Üí 10,000 verifica√ß√µes/dia
# A API gratuita do URLScan.io tinha limite de apenas 100 scans/dia
# Se precisares no futuro, basta descomentar esta fun√ß√£o
#
# async def check_urlscan(url: str) -> dict:
#     """Verifica URL no URLScan.io API (100 scans/dia)."""
#     if not settings.URLSCAN_API_KEY:
#         return {"checked": False, "error": "API key not configured"}
#     
#     api_url = "https://urlscan.io/api/v1/scan/"
#     headers = {"API-Key": settings.URLSCAN_API_KEY, "Content-Type": "application/json"}
#     payload = {"url": url, "visibility": "unlisted"}
#     
#     async with httpx.AsyncClient(timeout=15.0) as client:
#         response = await client.post(api_url, json=payload, headers=headers)
#         if response.status_code == 429:
#             return {"checked": False, "error": "rate_limit"}
#         response.raise_for_status()
#         data = response.json()
#         return {"checked": True, "scan_uuid": data.get("uuid"), "result_url": data.get("result")}


# ===========================================
# SSL/TLS VERIFICATION
# ===========================================

import ssl
import socket
from urllib.parse import urlparse

async def check_ssl(url: str) -> dict:
    """
    Verifica o certificado SSL/TLS do URL.
    
    Verifica:
    - Se usa HTTPS
    - Se certificado √© v√°lido
    - Se n√£o est√° expirado
    - Informa√ß√µes do certificado
    
    Returns:
        Dict com resultado da verifica√ß√£o SSL.
    """
    parsed = urlparse(url)
    hostname = parsed.hostname
    port = parsed.port or (443 if parsed.scheme == 'https' else 80)
    
    # Se n√£o √© HTTPS, j√° √© suspeito
    if parsed.scheme != 'https':
        logger.warning(f"‚ö†Ô∏è URL n√£o usa HTTPS: {url}")
        return {
            "checked": True,
            "has_ssl": False,
            "status": "suspicious",
            "reason": "Site n√£o usa HTTPS",
            "source": "ssl_checker"
        }
    
    try:
        # Criar contexto SSL
        context = ssl.create_default_context()
        
        # Conectar ao servidor
        loop = asyncio.get_event_loop()
        
        def get_cert():
            with socket.create_connection((hostname, port), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    return cert
        
        cert = await loop.run_in_executor(None, get_cert)
        
        if not cert:
            return {
                "checked": True,
                "has_ssl": False,
                "status": "suspicious",
                "reason": "N√£o foi poss√≠vel obter certificado",
                "source": "ssl_checker"
            }
        
        # Extrair informa√ß√µes do certificado
        issuer = dict(x[0] for x in cert.get('issuer', []))
        subject = dict(x[0] for x in cert.get('subject', []))
        not_after = cert.get('notAfter', '')
        
        # Verificar se expirou
        from datetime import datetime
        try:
            expiry_date = datetime.strptime(not_after, '%b %d %H:%M:%S %Y %Z')
            is_expired = expiry_date < datetime.now()
        except:
            is_expired = False
        
        if is_expired:
            logger.warning(f"‚ö†Ô∏è Certificado SSL expirado: {url}")
            return {
                "checked": True,
                "has_ssl": True,
                "status": "suspicious",
                "reason": "Certificado SSL expirado",
                "issuer": issuer.get('organizationName', 'Desconhecido'),
                "expiry": not_after,
                "source": "ssl_checker"
            }
        
        # Certificado v√°lido
        logger.info(f"‚úÖ SSL v√°lido para {hostname}")
        return {
            "checked": True,
            "has_ssl": True,
            "status": "safe",
            "issuer": issuer.get('organizationName', issuer.get('commonName', 'Desconhecido')),
            "subject": subject.get('commonName', hostname),
            "expiry": not_after,
            "source": "ssl_checker"
        }
        
    except ssl.SSLCertVerificationError as e:
        logger.error(f"‚ùå Certificado SSL inv√°lido: {e}")
        return {
            "checked": True,
            "has_ssl": True,
            "status": "malicious",
            "reason": "Certificado SSL inv√°lido ou n√£o confi√°vel",
            "error": str(e),
            "source": "ssl_checker"
        }
    except socket.timeout:
        logger.error(f"‚ùå Timeout ao verificar SSL")
        return {
            "checked": False,
            "status": "unknown",
            "error": "timeout",
            "source": "ssl_checker"
        }
    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar SSL: {e}")
        return {
            "checked": False,
            "status": "unknown",
            "error": str(e),
            "source": "ssl_checker"
        }


# ===========================================
# GROQ AI ANALYSIS
# ===========================================

async def get_ai_opinion(url: str, scan_results: dict) -> Optional[str]:
    """
    Obt√©m opini√£o da IA (Groq/Llama 3) sobre o URL.
    
    Returns:
        String com a opini√£o da IA ou None se falhar.
    """
    if not settings.GROQ_API_KEY:
        logger.warning("‚ö†Ô∏è Groq API key not configured")
        return None
    
    api_url = "https://api.groq.com/openai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {settings.GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Construir prompt com contexto dos scans
    google_result = scan_results.get("google_safe_browsing", {})
    ssl_result = scan_results.get("ssl_check", {})
    
    # Construir descri√ß√£o do SSL
    ssl_desc = "N√£o verificado"
    if ssl_result.get("checked"):
        if not ssl_result.get("has_ssl"):
            ssl_desc = "‚ö†Ô∏è Site n√£o usa HTTPS"
        elif ssl_result.get("status") == "malicious":
            ssl_desc = "‚ùå Certificado inv√°lido ou n√£o confi√°vel"
        elif ssl_result.get("status") == "suspicious":
            ssl_desc = f"‚ö†Ô∏è {ssl_result.get('reason', 'Problema com certificado')}"
        else:
            ssl_desc = f"‚úÖ V√°lido (emitido por {ssl_result.get('issuer', 'desconhecido')})"
    
    prompt = f"""Analisa este URL e d√° uma opini√£o concisa sobre a sua seguran√ßa.

URL: {url}

Resultados das verifica√ß√µes:
- Google Safe Browsing: {"Amea√ßa detectada: " + str(google_result.get("threats")) if google_result.get("is_threat") else "Nenhuma amea√ßa detectada" if google_result.get("checked") else "N√£o verificado"}
- Certificado SSL: {ssl_desc}

Responde em Portugu√™s de Portugal, de forma concisa (m√°ximo 2-3 frases).
Baseado nos resultados acima, indica se o URL √© SEGURO, SUSPEITO ou PERIGOSO.
Considera tamb√©m o dom√≠nio, estrutura do URL, e padr√µes comuns de phishing/scam.
Usa palavras como "seguro", "suspeito", "cautela" ou "perigoso" na tua resposta."""

    payload = {
        "model": settings.GROQ_MODEL,
        "messages": [
            {
                "role": "system",
                "content": "√âs um especialista em ciberseguran√ßa. Analisa URLs e d√° pareceres concisos sobre a sua seguran√ßa. Responde sempre em Portugu√™s de Portugal."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 200,
        "temperature": 0.3
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(api_url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            opinion = data["choices"][0]["message"]["content"].strip()
            
            logger.info(f"‚úÖ AI opinion generated for {url[:30]}...")
            return opinion
            
    except httpx.TimeoutException:
        logger.error("‚ùå Groq API timeout")
        return None
    except Exception as e:
        logger.error(f"‚ùå Groq API error: {e}")
        return None


# ===========================================
# MAIN CHECK FUNCTION
# ===========================================

async def check_url(url: str, force_recheck: bool = False) -> dict:
    """
    Verifica seguran√ßa de um URL.
    
    Arquitetura Stale-While-Revalidate:
    1. Se existe em cache e √© recente ‚Üí retorna imediatamente
    2. Se existe em cache mas √© antigo ‚Üí retorna e re-verifica em background
    3. Se n√£o existe ‚Üí verifica e guarda em cache
    
    Args:
        url: URL a verificar
        force_recheck: For√ßar nova verifica√ß√£o ignorando cache
    
    Returns:
        Dict com resultado da an√°lise.
    """
    # Normalizar URL
    url = normalize_url(url)
    url_hash = hash_url(url)
    
    logger.info(f"üîç Checking URL: {url[:50]}... (hash: {url_hash[:8]}...)")
    
    # 1. Verificar cache (se n√£o for√ßar recheck)
    if not force_recheck:
        cached = await get_cached_result(url_hash)
        
        if cached:
            last_check = datetime.fromisoformat(cached["last_check"].replace("Z", "+00:00"))
            age_seconds = (datetime.now(timezone.utc) - last_check).total_seconds()
            
            # Cache fresco (< 1 hora) ‚Üí retorna direto
            if age_seconds < settings.URL_CACHE_FRESH_SECONDS:
                logger.info(f"‚úÖ Fresh cache hit ({age_seconds:.0f}s old)")
                return {
                    "url": url,
                    "url_hash": url_hash,
                    "status": cached["status"],
                    "ai_opinion": cached.get("ai_opinion"),
                    "threat_details": cached.get("threat_details", {}),
                    "last_check": cached["last_check"],
                    "from_cache": True,
                    "cache_age_seconds": int(age_seconds)
                }
            
            # Cache antigo (> 1 hora mas < 24 horas) ‚Üí retorna e agenda recheck
            elif age_seconds < settings.URL_CACHE_TTL_SECONDS:
                logger.info(f"‚ö° Stale cache hit ({age_seconds:.0f}s old), triggering background recheck")
                
                # Agendar recheck em background (n√£o bloqueia)
                asyncio.create_task(_background_recheck(url, url_hash))
                
                return {
                    "url": url,
                    "url_hash": url_hash,
                    "status": cached["status"],
                    "ai_opinion": cached.get("ai_opinion"),
                    "threat_details": cached.get("threat_details", {}),
                    "last_check": cached["last_check"],
                    "from_cache": True,
                    "cache_age_seconds": int(age_seconds),
                    "recheck_triggered": True
                }
    
    # 2. Fazer verifica√ß√£o completa
    return await _perform_full_check(url, url_hash)


async def _perform_full_check(url: str, url_hash: str) -> dict:
    """Executa verifica√ß√£o completa do URL."""
    
    logger.info(f"üîÑ Performing full check for {url[:50]}...")
    
    # Executar verifica√ß√µes em paralelo
    google_task = check_google_safe_browsing(url)
    ssl_task = check_ssl(url)
    
    google_result, ssl_result = await asyncio.gather(
        google_task,
        ssl_task,
        return_exceptions=True
    )
    
    # Tratar exce√ß√µes
    if isinstance(google_result, Exception):
        logger.error(f"Google Safe Browsing exception: {google_result}")
        google_result = {"checked": False, "error": str(google_result)}
    
    if isinstance(ssl_result, Exception):
        logger.error(f"SSL check exception: {ssl_result}")
        ssl_result = {"checked": False, "status": "unknown", "error": str(ssl_result)}
    
    scan_results = {
        "google_safe_browsing": google_result,
        "ssl_check": ssl_result
    }
    
    # Determinar status baseado nos resultados
    status = _determine_status(google_result, ssl_result)
    
    # Obter opini√£o da IA
    ai_opinion = await get_ai_opinion(url, scan_results)
    
    # Se status √© SAFE mas IA detecta suspeita ‚Üí SUSPICIOUS
    if status == URLStatus.SAFE and ai_opinion:
        ai_lower = ai_opinion.lower()
        if any(word in ai_lower for word in ['suspeito', 'suspicious', 'cuidado', 'cautela', 'evitar', 'perigoso', 'phishing', 'scam', 'fraude']):
            status = URLStatus.SUSPICIOUS
    
    # Guardar em cache
    await save_to_cache(
        url_hash=url_hash,
        original_url=url,
        status=status,
        ai_opinion=ai_opinion,
        threat_details=scan_results
    )
    
    return {
        "url": url,
        "url_hash": url_hash,
        "status": status.value,
        "ai_opinion": ai_opinion,
        "threat_details": scan_results,
        "last_check": datetime.now(timezone.utc).isoformat(),
        "from_cache": False
    }


async def _background_recheck(url: str, url_hash: str):
    """Re-verifica URL em background (n√£o bloqueia resposta)."""
    try:
        logger.info(f"üîÑ Background recheck started for {url[:30]}...")
        await _perform_full_check(url, url_hash)
        logger.info(f"‚úÖ Background recheck completed for {url[:30]}...")
    except Exception as e:
        logger.error(f"‚ùå Background recheck failed: {e}")


def _determine_status(google_result: dict, ssl_result: dict) -> URLStatus:
    """Determina status final baseado nos resultados do Google Safe Browsing e SSL."""
    
    # Se Google Safe Browsing detectou amea√ßa ‚Üí MALICIOUS
    if google_result.get("is_threat"):
        return URLStatus.MALICIOUS
    
    # Se SSL √© inv√°lido/n√£o confi√°vel ‚Üí MALICIOUS
    if ssl_result.get("status") == "malicious":
        return URLStatus.MALICIOUS
    
    # Se SSL √© suspeito (expirado, sem HTTPS) ‚Üí SUSPICIOUS
    if ssl_result.get("status") == "suspicious":
        return URLStatus.SUSPICIOUS
    
    # Se Google verificou e n√£o encontrou nada + SSL ok ‚Üí SAFE
    if google_result.get("checked") and not google_result.get("is_threat"):
        if ssl_result.get("status") == "safe" or not ssl_result.get("checked"):
            return URLStatus.SAFE
    
    # Se nenhum scanner funcionou ‚Üí UNKNOWN
    if not google_result.get("checked") and not ssl_result.get("checked"):
        return URLStatus.UNKNOWN
    
    # Caso padr√£o
    return URLStatus.SAFE
